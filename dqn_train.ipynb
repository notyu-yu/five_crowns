{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCw04Buj6x2h"
      },
      "source": [
        "# Five Crowns Deep-Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwsXwXh360bJ"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WdPhYqtC6sZD"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "from deck import Card\n",
        "from player import Player\n",
        "from five_crowns import Game\n",
        "from greedy import GreedyPlayer\n",
        "from constants import GET_DISCARD, DRAW_CARD\n",
        "from scoring import score_hand, get_best_discard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah3q5AFz61cb"
      },
      "source": [
        "### Define the Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2honZ1hm65JP"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Q Network\n",
        "    \n",
        "    Args:\n",
        "        state_dim (int): dimension of the state space\n",
        "        action_dim (int): dimension of the action space\n",
        "        \n",
        "    Attributes:\n",
        "        fc (torch.nn.Sequential): fully connected layers\n",
        "\n",
        "    Methods:\n",
        "        forward: forward pass of the network\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the network\n",
        "        \n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor\n",
        "        \n",
        "        Returns:\n",
        "            torch.Tensor: output tensor\n",
        "        \"\"\"\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeLQuFxz66Dd"
      },
      "source": [
        "### Define the Memory Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2pekyat699v"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Replay Buffer\n",
        "    \n",
        "    Args:\n",
        "        capacity (int): capacity of the buffer\n",
        "        \n",
        "    Attributes:\n",
        "        buffer (collections.deque): buffer to store experiences\n",
        "        \n",
        "    Methods:\n",
        "        add: add an experience to the buffer\n",
        "        sample: sample a batch of experiences from the buffer\n",
        "        size: get the size of the buffer\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, experience):\n",
        "        \"\"\"\n",
        "        Add an experience to the buffer\n",
        "        \n",
        "        Args:\n",
        "            experience (tuple): experience to add to the buffer\n",
        "        \"\"\"\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample a batch of experiences from the buffer\n",
        "        \n",
        "        Args:\n",
        "            batch_size (int): size of the batch to sample\n",
        "        \n",
        "        Returns:\n",
        "            list: batch of experiences\n",
        "        \"\"\"\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def size(self):\n",
        "        \"\"\"\n",
        "        Get the size of the buffer\n",
        "        \n",
        "        Returns:\n",
        "            int: size of the buffer\n",
        "        \"\"\"\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "prGV73NMClLe"
      },
      "outputs": [],
      "source": [
        "def payoff(game, current_player):\n",
        "    \"\"\"\n",
        "    Calculates the reward for the player at this state\n",
        "\n",
        "    Returns:\n",
        "        int: The reward for the current player in the state\n",
        "    \"\"\"\n",
        "    if not game._go_out:\n",
        "        return 0\n",
        "    \n",
        "    # If the player went out, calculate the score\n",
        "    hand_score = score_hand(\n",
        "        game.get_player_hand(current_player), game\n",
        "    )\n",
        "    \n",
        "    return 1 if hand_score == 0 else -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mJ_6KTYOQ9bj"
      },
      "outputs": [],
      "source": [
        "def card_to_idx(suit, rank):\n",
        "    \"\"\"\n",
        "    Convert a card to an index in the state vector\n",
        "\n",
        "    Args:\n",
        "        suit (str): suit of the card\n",
        "        rank (int): rank of the card\n",
        "\n",
        "    Returns:\n",
        "        int: index of the card\n",
        "    \"\"\"\n",
        "    suit_dict = {\n",
        "        'Clubs': 0,\n",
        "        'Diamonds': 1,\n",
        "        'Hearts': 2,\n",
        "        'Spades': 3,\n",
        "        'Stars': 4,\n",
        "        'J': 5\n",
        "    }\n",
        "\n",
        "    return 11 * suit_dict[suit] + (rank if suit != \"J\" else 0) - 3\n",
        "\n",
        "def idx_to_card(idx):\n",
        "    \"\"\"\n",
        "    Convert an index to a card in the state vector\n",
        "    \n",
        "    Args:\n",
        "        idx (int): index of the card\n",
        "        \n",
        "    Returns:\n",
        "        Card: card corresponding to the index\n",
        "    \"\"\"\n",
        "    suit_dict = {\n",
        "        0: 'Clubs',\n",
        "        1: 'Diamonds',\n",
        "        2: 'Hearts',\n",
        "        3: 'Spades',\n",
        "        4: 'Stars',\n",
        "        5: 'J'\n",
        "    }\n",
        "\n",
        "    suit = suit_dict[idx // 11]\n",
        "    if suit == \"J\":\n",
        "        rank = 50\n",
        "    else:\n",
        "        rank = (idx % 11) + 3\n",
        "\n",
        "    return Card(rank, suit)\n",
        "\n",
        "def encode_state(num_players, full_deck, player_deck, discard_card, gone_out_status):\n",
        "    \"\"\"\n",
        "    Encode the state of the game into a vector\n",
        "    \n",
        "    Args:\n",
        "        num_players (int): number of players in the game\n",
        "        full_deck (list): full deck of cards\n",
        "        player_deck (list): deck of the player\n",
        "        discard_card (Card): card that was discarded\n",
        "        gone_out_status (bool): whether the player has gone out\n",
        "        \n",
        "    Returns:\n",
        "        np.array: encoded state of the game\n",
        "    \"\"\"\n",
        "    num_players = num_players\n",
        "\n",
        "    deck = set(full_deck)\n",
        "\n",
        "    encoded_deck = np.zeros(len(deck))\n",
        "    for card in cards:\n",
        "        card_idx = card_to_idx(card.suit(), card.rank())\n",
        "        encoded_deck[card_idx] += 1\n",
        "\n",
        "    # Encode discard card as (rank, suit)\n",
        "    discard_card_encoded = np.zeros(len(deck))\n",
        "    if discard_card is not None:\n",
        "        discard_idx = card_to_idx(discard_card.suit(), discard_card.rank())\n",
        "        discard_card_encoded[discard_idx] = 1\n",
        "\n",
        "    # Gone out status\n",
        "    gone_out_status_encoded = int(gone_out_status)\n",
        "\n",
        "    return np.concatenate([\n",
        "        encoded_deck,\n",
        "        discard_card_encoded,\n",
        "        [gone_out_status_encoded]\n",
        "    ])\n",
        "\n",
        "def inference(game, hand, discard_card, policy_net):\n",
        "    \"\"\"\n",
        "    Inference function for the policy network\n",
        "    \n",
        "    Args:\n",
        "        game (Game): game object\n",
        "        hand (list): hand of the player\n",
        "        discard_card (Card): card that was discarded\n",
        "        policy_net (DQN): policy network\n",
        "        \n",
        "    Returns:\n",
        "        Card: card to play\n",
        "    \"\"\"\n",
        "    encoded_state = encode_state(game.num_players(), game.get_full_deck()._cards, hand, discard_card, game._go_out)\n",
        "    model = policy_net(encoded_state.shape[0], 56)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(encoded_state).item()\n",
        "\n",
        "        sorted_list = [(output[i], idx_to_card(i)) for i in range(len(output))]\n",
        "        sorted_list.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "\n",
        "        for i in range(len(sorted_list)):\n",
        "            if sorted_list[i][1] in hand:\n",
        "                return sorted_list[i][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the DQNPlayer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Koa2H1ecSiEL"
      },
      "outputs": [],
      "source": [
        "class DQNPlayer(Player):\n",
        "    \"\"\"\n",
        "    DQN player always takes action that minimize score for turn\n",
        "\n",
        "    Args:\n",
        "        player_id (int): player id\n",
        "        policy_net (DQN): policy network\n",
        "\n",
        "    Attributes:\n",
        "        prev_discard (Card): previous discard card\n",
        "        epsilon (float): epsilon for epsilon-greedy policy\n",
        "        epsilon_decay (float): decay rate for epsilon\n",
        "        min_epsilon (float): minimum epsilon\n",
        "        prev_action (int): previous action\n",
        "        policy_net (DQN): policy network\n",
        "\n",
        "    Methods:\n",
        "        draw_phase: draw phase of the player\n",
        "        discard_phase: discard phase of the player\n",
        "    \"\"\"\n",
        "    def __init__(self, player_id, policy_net):\n",
        "        super().__init__(player_id)\n",
        "        self.prev_discard = None\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.min_epsilon = 0.1\n",
        "        self.prev_action = None\n",
        "        self.policy_net = policy_net\n",
        "\n",
        "    def draw_phase(self, game):\n",
        "        \"\"\"\n",
        "        Draw phase of the player\n",
        "        \n",
        "        Args:\n",
        "            game (Game): game object\n",
        "            \n",
        "        Returns:\n",
        "            int: action to take\n",
        "        \"\"\"\n",
        "        # Get best score if we take discard\n",
        "        new_card = game.get_discard_pile()[-1]\n",
        "        temp_hand = self.hand + [new_card]\n",
        "        _, discard_score = get_best_discard(temp_hand,game,excluded_discard=new_card)\n",
        "\n",
        "        # Get best expected score if we draw random\n",
        "        remaining_deck = copy.deepcopy(game.get_full_deck().get_cards())\n",
        "        draw_scores = []\n",
        "        for card in game.get_discard_pile() + self.hand:\n",
        "            remaining_deck.remove(card)\n",
        "        for card in remaining_deck:\n",
        "            temp_hand = self.hand + [card]\n",
        "            _, draw_score = get_best_discard(temp_hand, game)\n",
        "            draw_scores.append(draw_score)\n",
        "        expected_draw_score = sum(draw_scores)/len(draw_scores)\n",
        "\n",
        "        # Take action with better expected score\n",
        "        if discard_score < expected_draw_score:\n",
        "            self.prev_discard = game.get_discard_pile()[-1]\n",
        "            return GET_DISCARD\n",
        "        self.prev_discard = None\n",
        "        return DRAW_CARD\n",
        "\n",
        "    def discard_phase(self, game):\n",
        "        \"\"\"\n",
        "        Discard phase of the player\n",
        "        \n",
        "        Args:\n",
        "            game (Game): game object\n",
        "            \n",
        "        Returns:\n",
        "            Card: card to discard\n",
        "        \"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            action = random.choice([card for card in self.hand if card != self.prev_discard])\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                action = inference(game, self.hand, self.prev_discard, self.policy_net)\n",
        "\n",
        "        encoded_action = np.zeros(56)\n",
        "        idx = card_to_idx(action.suit(), action.rank())\n",
        "        encoded_action[idx] = 1\n",
        "        self.prev_action = idx\n",
        "        return action\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up the Environment and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dNWPTzBo_m8J"
      },
      "outputs": [],
      "source": [
        "# Set up the environment\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "agents = 4\n",
        "state_dim = 113\n",
        "action_dim = 56\n",
        "\n",
        "policy_net = DQN(state_dim, action_dim).to(device)\n",
        "target_net = DQN(state_dim, action_dim).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "# Set up the players and game\n",
        "dqn_player = DQNPlayer(0, policy_net)\n",
        "players = [dqn_player] + [GreedyPlayer(i) for i in range(1, agents)]\n",
        "\n",
        "env = Game(players)\n",
        "env.initialize_game()\n",
        "\n",
        "state_encoded = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
        "\n",
        "# Set up the optimizer, buffer, and hyperparameters\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
        "buffer = ReplayBuffer(10000)\n",
        "batch_size = 64\n",
        "gamma = 0.99\n",
        "target_update_freq = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgFWGvHJVNkf"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQCgN6l1VM1t",
        "outputId": "2d603ebf-9fb9-46d8-a185-e56ef0a40eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 51, Loss: 0.38696005940437317\n",
            "Episode 101, Loss: 0.3487950563430786\n",
            "Episode 101, Loss: 0.4406189024448395\n",
            "Episode 151, Loss: 0.39293473958969116\n",
            "Episode 151, Loss: 0.45359545946121216\n",
            "Episode 201, Loss: 0.325755774974823\n",
            "Episode 201, Loss: 0.33951467275619507\n",
            "Episode 251, Loss: 0.3827046751976013\n",
            "Episode 251, Loss: 0.44889533519744873\n",
            "Episode 251, Loss: 0.39683130383491516\n",
            "Episode 301, Loss: 0.3639007806777954\n",
            "Episode 301, Loss: 0.3315916657447815\n",
            "Episode 351, Loss: 0.37709981203079224\n",
            "Episode 351, Loss: 0.3358755111694336\n",
            "Episode 401, Loss: 0.3760087490081787\n",
            "Episode 401, Loss: 0.3506568968296051\n",
            "Episode 451, Loss: 0.4215092658996582\n",
            "Episode 451, Loss: 0.32699063420295715\n",
            "Episode 501, Loss: 0.46768128871917725\n",
            "Episode 501, Loss: 0.33663687109947205\n",
            "Episode 501, Loss: 0.44248345494270325\n",
            "Episode 551, Loss: 0.427690327167511\n",
            "Episode 551, Loss: 0.4267599582672119\n",
            "Episode 601, Loss: 0.4447503983974457\n",
            "Episode 601, Loss: 0.4428597688674927\n",
            "Episode 601, Loss: 0.4026695489883423\n",
            "Episode 651, Loss: 0.38917648792266846\n",
            "Episode 651, Loss: 0.38674816489219666\n",
            "Episode 701, Loss: 0.496293842792511\n",
            "Episode 701, Loss: 0.3695515990257263\n",
            "Episode 751, Loss: 0.36771780252456665\n",
            "Episode 751, Loss: 0.4612412750720978\n",
            "Episode 751, Loss: 0.398869127035141\n",
            "Episode 801, Loss: 0.3959977626800537\n",
            "Episode 801, Loss: 0.3962963819503784\n",
            "Episode 851, Loss: 0.3430118262767792\n",
            "Episode 851, Loss: 0.37247878313064575\n",
            "Episode 901, Loss: 0.45236802101135254\n",
            "Episode 901, Loss: 0.3930938243865967\n",
            "Episode 951, Loss: 0.41609734296798706\n",
            "Episode 951, Loss: 0.3619345426559448\n",
            "Episode 951, Loss: 0.4641513228416443\n",
            "Ending loss:  0.4281455874443054\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "for episode in range(1000):\n",
        "    \n",
        "    # Initialize the environment\n",
        "    players = [dqn_player] + [GreedyPlayer(i) for i in range(1, agents)]\n",
        "    env = Game(players)\n",
        "    env.initialize_game()\n",
        "    state = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
        "    done = False\n",
        "\n",
        "    # Play the game while it is not over\n",
        "    while not done:\n",
        "        for _ in range(agents):\n",
        "          env.play_round()\n",
        "          if env.is_game_over():\n",
        "            reward = payoff(env, 0)\n",
        "            done = True\n",
        "          else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        # Get the next state\n",
        "        next_state = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
        "\n",
        "        buffer.add((state, env._players[0].prev_action, reward, next_state, done))\n",
        "        state = next_state\n",
        "\n",
        "        # Sample a batch of experiences from the buffer\n",
        "        if buffer.size() >= batch_size:\n",
        "            batch = buffer.sample(batch_size)\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "            states = torch.tensor(states, dtype=torch.float32).to(device)\n",
        "            actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
        "            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "            next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
        "            dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
        "\n",
        "            # Calculate the q-values and loss\n",
        "            q_values = policy_net(states)\n",
        "            q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
        "            next_q_values = target_net(next_states).max(1)[0]\n",
        "            target = rewards + (gamma * next_q_values * (1 - dones))\n",
        "\n",
        "            loss = nn.MSELoss()(q_values, target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if episode % 50 == 0:\n",
        "              print(f\"Episode {episode + 1}, Loss: {loss}\")\n",
        "\n",
        "    if episode % target_update_freq == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "print(\"Ending loss: \", loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xRCjkJ5SVQoL"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(policy_net.state_dict(), \"five_crowns_dqn.pth\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
