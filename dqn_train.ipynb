{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCw04Buj6x2h"
      },
      "source": [
        "# Five Crowns Deep-Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwsXwXh360bJ"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WdPhYqtC6sZD"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "from copy import deepcopy\n",
        "from five_crowns import Game\n",
        "from greedy import GreedyPlayer\n",
        "from scoring import score_hand\n",
        "from state import State"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah3q5AFz61cb"
      },
      "source": [
        "### Define the Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2honZ1hm65JP"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeLQuFxz66Dd"
      },
      "source": [
        "### Define the Memory Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "A2pekyat699v"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "prGV73NMClLe"
      },
      "outputs": [],
      "source": [
        "def payoff(game, current_player):\n",
        "    \"\"\"\n",
        "    Calculates the reward for the player at this state\n",
        "\n",
        "    Returns:\n",
        "        int: The reward for the current player in the state\n",
        "    \"\"\"\n",
        "    if not game._go_out:\n",
        "        return 0\n",
        "    # print(self.game.get_player_hand(self.curr_player_id))\n",
        "    hand_score = score_hand(\n",
        "        game.get_player_hand(current_player), game\n",
        "    )\n",
        "    return 1 if hand_score == 0 else -1\n",
        "\n",
        "def successor(self, game, action, num_players):\n",
        "    \"\"\"\n",
        "    Returns the successor state given the action from the current state\n",
        "\n",
        "    Args:\n",
        "        - action: The action to take from the current state\n",
        "\n",
        "    Returns:\n",
        "        - State: The new state after taking the action\n",
        "    \"\"\"\n",
        "    # Make a copy for the successor state\n",
        "    new_state = deepcopy(game)\n",
        "\n",
        "    # Get the next player\n",
        "    next_player_id = (game.get_active_player() + 1) % num_players\n",
        "    new_state._active_player = next_player_id\n",
        "\n",
        "    # Get the actions\n",
        "    first_action = action[0]\n",
        "    second_action = action[1]\n",
        "\n",
        "    # Execute the first action\n",
        "    added_card = None\n",
        "    if first_action == \"deck\":\n",
        "        added_card = new_state.get_deck().draw()\n",
        "        new_state.get_player_hand(game.get_active_player()).append(added_card)\n",
        "    elif first_action == \"discard\":\n",
        "        added_card = new_state.get_discard_pile().pop()\n",
        "        new_state.get_player_hand(game.get_active_player()).append(added_card)\n",
        "\n",
        "    # Execute the second action\n",
        "    if second_action == None:\n",
        "        new_state.get_player_hand(self.curr_player_id).remove(added_card)\n",
        "        new_state.get_discard_pile().append(added_card)\n",
        "    else:\n",
        "        new_state.get_player_hand(\n",
        "            self.curr_player_id).remove(second_action)\n",
        "        new_state.get_discard_pile().append(second_action)\n",
        "\n",
        "    # Check the hand score and update game ending conditions\n",
        "    # print(new_state.get_player_hand(self.curr_player_id))\n",
        "    hand_score = score_hand(\n",
        "        new_state.get_player_hand(self.curr_player_id), new_state\n",
        "    )\n",
        "    if hand_score == 0:\n",
        "        new_state._go_out = True\n",
        "        if new_state._remaining_players == new_state.num_players():\n",
        "            new_state._go_out_player = self.curr_player_id\n",
        "        new_state._remaining_players -= 1\n",
        "        if new_state._remaining_players == 0:\n",
        "            new_state._game_over = True\n",
        "\n",
        "    return State(new_state)\n",
        "\n",
        "def get_actions(self):\n",
        "      \"\"\"\n",
        "      Returns all possible actions from the current state\n",
        "      \"\"\"\n",
        "      if self.is_root:\n",
        "          actions = [(\"root\", c) for c in self.curr_player_hand if c != self.root_card]\n",
        "\n",
        "      else:\n",
        "          # initialize actions list\n",
        "          actions = []\n",
        "\n",
        "          if self.discard_pile_card:\n",
        "              #actions.append((\"discard\", self.discard_pile_card))\n",
        "              for card in self.curr_player_hand:\n",
        "                  actions.append((\"discard\", card))\n",
        "          if game.get_deck():\n",
        "              actions.append((\"deck\", None))\n",
        "              for card in self.curr_player_hand:\n",
        "                  actions.append((\"deck\", card))\n",
        "\n",
        "      return actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mJ_6KTYOQ9bj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from deck import Card\n",
        "\n",
        "def card_to_idx(suit, rank):\n",
        "  suit_dict = {\n",
        "      'Clubs': 0,\n",
        "      'Diamonds': 1,\n",
        "      'Hearts': 2,\n",
        "      'Spades': 3,\n",
        "      'Stars': 4,\n",
        "      'J': 5\n",
        "  }\n",
        "\n",
        "  return 11 * suit_dict[suit] + (rank if suit != \"J\" else 0) - 3\n",
        "\n",
        "def idx_to_card(idx):\n",
        "    suit_dict = {\n",
        "        0: 'Clubs',\n",
        "        1: 'Diamonds',\n",
        "        2: 'Hearts',\n",
        "        3: 'Spades',\n",
        "        4: 'Stars',\n",
        "        5: 'J'\n",
        "    }\n",
        "\n",
        "    suit = suit_dict[idx // 11]\n",
        "    if suit == \"J\":\n",
        "        rank = 50\n",
        "    else:\n",
        "        rank = (idx % 11) + 3\n",
        "\n",
        "    return Card(rank, suit)\n",
        "\n",
        "def encode_state(num_players, full_deck, player_deck, discard_card, gone_out_status):\n",
        "    num_players = num_players\n",
        "\n",
        "    deck = set(full_deck)\n",
        "\n",
        "    encoded_deck = np.zeros(len(deck))\n",
        "    for idx, card in enumerate(player_deck):\n",
        "        card_idx = card_to_idx(card.suit(), card.rank())\n",
        "        encoded_deck[card_idx] += 1\n",
        "\n",
        "    # Encode discard card as (rank, suit)\n",
        "    discard_card_encoded = np.zeros(len(deck))\n",
        "    if discard_card is not None:\n",
        "        discard_idx = card_to_idx(discard_card.suit(), discard_card.rank())\n",
        "        discard_card_encoded[discard_idx] = 1\n",
        "\n",
        "    # Gone out status\n",
        "    gone_out_status_encoded = int(gone_out_status)\n",
        "\n",
        "    return np.concatenate([\n",
        "        encoded_deck,\n",
        "        discard_card_encoded,\n",
        "        [gone_out_status_encoded]\n",
        "    ])\n",
        "\n",
        "def inference(game, hand, discard_card, policy_net):\n",
        "    encoded_state = encode_state(game.num_players(), game.get_full_deck()._cards, hand, discard_card, game._go_out)\n",
        "    model = policy_net(encoded_state.shape[0], 56)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(encoded_state).item()\n",
        "\n",
        "        sorted_list = [(output[i], idx_to_card(i)) for i in range(len(output))]\n",
        "        sorted_list.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "\n",
        "        for i in range(len(sorted_list)):\n",
        "            if sorted_list[i][1] in hand:\n",
        "                return sorted_list[i][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Koa2H1ecSiEL"
      },
      "outputs": [],
      "source": [
        "from player import Player\n",
        "from scoring import get_best_discard\n",
        "from constants import GET_DISCARD, DRAW_CARD\n",
        "import copy\n",
        "\n",
        "class DQNPlayer(Player):\n",
        "    \"\"\"\n",
        "    DQN player always takes action that minimize score for turn\n",
        "    \"\"\"\n",
        "    def __init__(self, player_id, policy_net):\n",
        "        super().__init__(player_id)\n",
        "        self.prev_discard = None\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.min_epsilon = 0.1\n",
        "        self.prev_action = None\n",
        "        self.policy_net = policy_net\n",
        "\n",
        "    def draw_phase(self, game):\n",
        "        # Get best score if we take discard\n",
        "        new_card = game.get_discard_pile()[-1]\n",
        "        temp_hand = self.hand + [new_card]\n",
        "        _, discard_score = get_best_discard(temp_hand,game,excluded_discard=new_card)\n",
        "\n",
        "        # Get best expected score if we draw random\n",
        "        remaining_deck = copy.deepcopy(game.get_full_deck().get_cards())\n",
        "        draw_scores = []\n",
        "        for card in game.get_discard_pile() + self.hand:\n",
        "            remaining_deck.remove(card)\n",
        "        for card in remaining_deck:\n",
        "            temp_hand = self.hand + [card]\n",
        "            _, draw_score = get_best_discard(temp_hand, game)\n",
        "            draw_scores.append(draw_score)\n",
        "        expected_draw_score = sum(draw_scores)/len(draw_scores)\n",
        "\n",
        "        # Take action with better expected score\n",
        "        if discard_score < expected_draw_score:\n",
        "            self.prev_discard = game.get_discard_pile()[-1]\n",
        "            return GET_DISCARD\n",
        "        self.prev_discard = None\n",
        "        return DRAW_CARD\n",
        "\n",
        "    def discard_phase(self, game):\n",
        "        if random.random() < self.epsilon:\n",
        "            action = random.choice([card for card in self.hand if card != self.prev_discard])\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                action = inference(game, self.hand, self.prev_discard, self.policy_net)\n",
        "\n",
        "        epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "        encoded_action = np.zeros(56)\n",
        "        idx = card_to_idx(action.suit(), action.rank())\n",
        "        encoded_action[idx] = 1\n",
        "        self.prev_action = idx\n",
        "        return action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dNWPTzBo_m8J"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "agents = 4\n",
        "state_dim = 113\n",
        "action_dim = 56\n",
        "\n",
        "policy_net = DQN(state_dim, action_dim).to(device)\n",
        "target_net = DQN(state_dim, action_dim).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "dqn_player = DQNPlayer(0, policy_net)\n",
        "players = [dqn_player] + [GreedyPlayer(i) for i in range(1, agents)]\n",
        "\n",
        "env = Game(players)\n",
        "env.initialize_game()\n",
        "\n",
        "state_encoded = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
        "buffer = ReplayBuffer(10000)\n",
        "batch_size = 64\n",
        "gamma = 0.99\n",
        "target_update_freq = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgFWGvHJVNkf"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQCgN6l1VM1t",
        "outputId": "2d603ebf-9fb9-46d8-a185-e56ef0a40eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 51, Loss: 0.38696005940437317\n",
            "Episode 101, Loss: 0.3487950563430786\n",
            "Episode 101, Loss: 0.4406189024448395\n",
            "Episode 151, Loss: 0.39293473958969116\n",
            "Episode 151, Loss: 0.45359545946121216\n",
            "Episode 201, Loss: 0.325755774974823\n",
            "Episode 201, Loss: 0.33951467275619507\n",
            "Episode 251, Loss: 0.3827046751976013\n",
            "Episode 251, Loss: 0.44889533519744873\n",
            "Episode 251, Loss: 0.39683130383491516\n",
            "Episode 301, Loss: 0.3639007806777954\n",
            "Episode 301, Loss: 0.3315916657447815\n",
            "Episode 351, Loss: 0.37709981203079224\n",
            "Episode 351, Loss: 0.3358755111694336\n",
            "Episode 401, Loss: 0.3760087490081787\n",
            "Episode 401, Loss: 0.3506568968296051\n",
            "Episode 451, Loss: 0.4215092658996582\n",
            "Episode 451, Loss: 0.32699063420295715\n",
            "Episode 501, Loss: 0.46768128871917725\n",
            "Episode 501, Loss: 0.33663687109947205\n",
            "Episode 501, Loss: 0.44248345494270325\n",
            "Episode 551, Loss: 0.427690327167511\n",
            "Episode 551, Loss: 0.4267599582672119\n",
            "Episode 601, Loss: 0.4447503983974457\n",
            "Episode 601, Loss: 0.4428597688674927\n",
            "Episode 601, Loss: 0.4026695489883423\n",
            "Episode 651, Loss: 0.38917648792266846\n",
            "Episode 651, Loss: 0.38674816489219666\n",
            "Episode 701, Loss: 0.496293842792511\n",
            "Episode 701, Loss: 0.3695515990257263\n",
            "Episode 751, Loss: 0.36771780252456665\n",
            "Episode 751, Loss: 0.4612412750720978\n",
            "Episode 751, Loss: 0.398869127035141\n",
            "Episode 801, Loss: 0.3959977626800537\n",
            "Episode 801, Loss: 0.3962963819503784\n",
            "Episode 851, Loss: 0.3430118262767792\n",
            "Episode 851, Loss: 0.37247878313064575\n",
            "Episode 901, Loss: 0.45236802101135254\n",
            "Episode 901, Loss: 0.3930938243865967\n",
            "Episode 951, Loss: 0.41609734296798706\n",
            "Episode 951, Loss: 0.3619345426559448\n",
            "Episode 951, Loss: 0.4641513228416443\n",
            "Ending loss:  0.4281455874443054\n"
          ]
        }
      ],
      "source": [
        "for episode in range(1000):\n",
        "    players = [dqn_player] + [GreedyPlayer(i) for i in range(1, agents)]\n",
        "    env = Game(players)\n",
        "    env.initialize_game()\n",
        "    state = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
        "    done = False\n",
        "    while not done:\n",
        "        for _ in range(agents):\n",
        "          env.play_round()\n",
        "          if env.is_game_over():\n",
        "            reward = payoff(env, 0)\n",
        "            done = True\n",
        "          else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        next_state = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
        "\n",
        "        buffer.add((state, env._players[0].prev_action, reward, next_state, done))\n",
        "        state = next_state\n",
        "\n",
        "        if buffer.size() >= batch_size:\n",
        "            batch = buffer.sample(batch_size)\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "            states = torch.tensor(states, dtype=torch.float32).to(device)\n",
        "            actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
        "            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "            next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
        "            dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
        "\n",
        "            # print(states.shape, actions.shape)\n",
        "            q_values = policy_net(states)\n",
        "            # print(q_values.shape)\n",
        "            # print(actions.shape)\n",
        "            q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
        "            next_q_values = target_net(next_states).max(1)[0]\n",
        "            target = rewards + (gamma * next_q_values * (1 - dones))\n",
        "\n",
        "            loss = nn.MSELoss()(q_values, target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if episode % 50 == 0:\n",
        "              print(f\"Episode {episode + 1}, Loss: {loss}\")\n",
        "\n",
        "    if episode % target_update_freq == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "print(\"Ending loss: \", loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xRCjkJ5SVQoL"
      },
      "outputs": [],
      "source": [
        "torch.save(policy_net.state_dict(), \"five_crowns_dqn.pth\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
