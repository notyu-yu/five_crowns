{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCw04Buj6x2h"
   },
   "source": [
    "# Five Crowns Deep-Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwsXwXh360bJ"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WdPhYqtC6sZD"
   },
   "outputs": [],
   "source": [
    "#Reference: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "from copy import deepcopy\n",
    "from five_crowns import Game\n",
    "from greedy import GreedyPlayer\n",
    "from scoring import score_hand\n",
    "from state import State\n",
    "from dqn import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ah3q5AFz61cb"
   },
   "source": [
    "### Define the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeLQuFxz66Dd"
   },
   "source": [
    "### Define the Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A2pekyat699v"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "prGV73NMClLe"
   },
   "outputs": [],
   "source": [
    "def payoff(game, current_player):\n",
    "    \"\"\"\n",
    "    Calculates the reward for the player at this state\n",
    "\n",
    "    Returns:\n",
    "        int: The reward for the current player in the state\n",
    "    \"\"\"\n",
    "    if not game._go_out:\n",
    "        return 0\n",
    "    # print(self.game.get_player_hand(self.curr_player_id))\n",
    "    hand_score = score_hand(\n",
    "        game.get_player_hand(current_player), game\n",
    "    )\n",
    "    return 1 if hand_score == 0 else -1 * hand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mJ_6KTYOQ9bj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from deck import Card\n",
    "\n",
    "def card_to_idx(suit, rank):\n",
    "  suit_dict = {\n",
    "      'Clubs': 0,\n",
    "      'Diamonds': 1,\n",
    "      'Hearts': 2,\n",
    "      'Spades': 3,\n",
    "      'Stars': 4,\n",
    "      'J': 5\n",
    "  }\n",
    "\n",
    "  return 11 * suit_dict[suit] + (rank if suit != \"J\" else 0) - 3\n",
    "\n",
    "def idx_to_card(idx):\n",
    "    suit_dict = {\n",
    "        0: 'Clubs',\n",
    "        1: 'Diamonds',\n",
    "        2: 'Hearts',\n",
    "        3: 'Spades',\n",
    "        4: 'Stars',\n",
    "        5: 'J'\n",
    "    }\n",
    "\n",
    "    suit = suit_dict[idx // 11]\n",
    "    if suit == \"J\":\n",
    "        rank = 50\n",
    "    else:\n",
    "        rank = (idx % 11) + 3\n",
    "\n",
    "    return Card(rank, suit)\n",
    "\n",
    "def encode_state(num_players, full_deck, player_deck, discard_card, gone_out_status):\n",
    "    num_players = num_players\n",
    "\n",
    "    deck = set(full_deck)\n",
    "\n",
    "    encoded_deck = np.zeros(len(deck))\n",
    "    for idx, card in enumerate(player_deck):\n",
    "        card_idx = card_to_idx(card.suit(), card.rank())\n",
    "        encoded_deck[card_idx] += 1\n",
    "\n",
    "    # Encode discard card as (rank, suit)\n",
    "    discard_card_encoded = np.zeros(len(deck))\n",
    "    if discard_card is not None:\n",
    "        discard_idx = card_to_idx(discard_card.suit(), discard_card.rank())\n",
    "        discard_card_encoded[discard_idx] = 1\n",
    "\n",
    "    # Gone out status\n",
    "    gone_out_status_encoded = int(gone_out_status)\n",
    "\n",
    "    return np.concatenate([\n",
    "        encoded_deck,\n",
    "        discard_card_encoded,\n",
    "        [gone_out_status_encoded]\n",
    "    ])\n",
    "\n",
    "def inference(game, hand, discard_card, policy_net):\n",
    "    encoded_state = encode_state(game.num_players(), game.get_full_deck()._cards, hand, discard_card, game._go_out)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = policy_net(torch.Tensor(encoded_state).to(device)).to(\"cpu\").numpy()\n",
    "\n",
    "        sorted_list = [(output[i], idx_to_card(i)) for i in range(len(output))]\n",
    "        sorted_list.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        for _,card in sorted_list:\n",
    "            if card in hand and card != discard_card:\n",
    "                return card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Koa2H1ecSiEL"
   },
   "outputs": [],
   "source": [
    "from player import Player\n",
    "from scoring import get_best_discard\n",
    "from constants import GET_DISCARD, DRAW_CARD\n",
    "import copy\n",
    "\n",
    "class DQNPlayer(Player):\n",
    "    \"\"\"\n",
    "    DQN player always takes action that minimize score for turn\n",
    "    \"\"\"\n",
    "    def __init__(self, player_id, policy_net):\n",
    "        super().__init__(player_id)\n",
    "        self.prev_discard = None\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.998\n",
    "        self.min_epsilon = 0.05\n",
    "        self.prev_action = None\n",
    "        self.policy_net = policy_net\n",
    "        self.policy_net.eval()\n",
    "\n",
    "    def draw_phase(self, game):\n",
    "        # Get best score if we take discard\n",
    "        new_card = game.get_discard_pile()[-1]\n",
    "        temp_hand = self.hand + [new_card]\n",
    "        _, discard_score = get_best_discard(temp_hand,game,excluded_discard=new_card)\n",
    "\n",
    "        # Get best expected score if we draw random\n",
    "        remaining_deck = copy.deepcopy(game.get_full_deck().get_cards())\n",
    "        draw_scores = []\n",
    "        for card in game.get_discard_pile() + self.hand:\n",
    "            remaining_deck.remove(card)\n",
    "        for card in remaining_deck:\n",
    "            temp_hand = self.hand + [card]\n",
    "            _, draw_score = get_best_discard(temp_hand, game)\n",
    "            draw_scores.append(draw_score)\n",
    "        expected_draw_score = sum(draw_scores)/len(draw_scores)\n",
    "\n",
    "        # Take action with better expected score\n",
    "        if discard_score < expected_draw_score:\n",
    "            self.prev_discard = game.get_discard_pile()[-1]\n",
    "            return GET_DISCARD\n",
    "        self.prev_discard = None\n",
    "        return DRAW_CARD\n",
    "\n",
    "    def discard_phase(self, game):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice([card for card in self.hand if card != self.prev_discard])\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = inference(game, self.hand, self.prev_discard, self.policy_net)\n",
    "\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        idx = card_to_idx(action.suit(), action.rank())\n",
    "        self.prev_action = idx\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dNWPTzBo_m8J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agents = 4\n",
    "state_dim = 113\n",
    "action_dim = 56\n",
    "epoch=3\n",
    "lr = 1e-4\n",
    "tau = 0.005\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnK0PDE_pwJ8",
    "outputId": "1782bab7-7c25-4551-85f6-82e8e4efa098"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net = DQN(state_dim, action_dim).to(device)\n",
    "policy_net.load_state_dict(torch.load(f'five_crowns_dqn_{epoch}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2FNRG0_fpv8W"
   },
   "outputs": [],
   "source": [
    "target_net = DQN(state_dim, action_dim).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "dqn_player = DQNPlayer(0, policy_net)\n",
    "players = [dqn_player] + [GreedyPlayer(i) for i in range(1, agents)]\n",
    "\n",
    "env = Game(players,epoch=epoch)\n",
    "env.initialize_game()\n",
    "\n",
    "state_encoded = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr, amsgrad=True)\n",
    "buffer = ReplayBuffer(10000)\n",
    "batch_size = 128\n",
    "gamma = 0.99\n",
    "target_update_freq = 100\n",
    "loss = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgFWGvHJVNkf"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQCgN6l1VM1t",
    "outputId": "ba21aefe-e3b3-43d3-f12b-8e62db965b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Loss: None, Win Rate: 1.0, Epsilon: 0.996004\n",
      "Episode 1001, Loss: 7.0244550704956055, Win Rate: 0.1572349570200573, Epsilon: 0.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(agents):\n\u001b[0;32m---> 10\u001b[0m       env\u001b[38;5;241m.\u001b[39mplay_round()\n\u001b[1;32m     11\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mis_game_over():\n\u001b[1;32m     12\u001b[0m         reward \u001b[38;5;241m=\u001b[39m payoff(env, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/mnt/dl_mount/game_project/five_crowns.py:159\u001b[0m, in \u001b[0;36mGame.play_round\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_go_out:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Draw Phase\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# print(\"Card in discard pile: \", self._discard_pile[-1])\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m player\u001b[38;5;241m.\u001b[39mdraw_phase(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m GET_DISCARD:\n\u001b[1;32m    160\u001b[0m         player\u001b[38;5;241m.\u001b[39mhand\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discard_pile\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/dl_mount/game_project/greedy.py:28\u001b[0m, in \u001b[0;36mGreedyPlayer.draw_phase\u001b[0;34m(self, game)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m card \u001b[38;5;129;01min\u001b[39;00m remaining_deck:\n\u001b[1;32m     27\u001b[0m     temp_hand \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhand \u001b[38;5;241m+\u001b[39m [card]\n\u001b[0;32m---> 28\u001b[0m     _, draw_score \u001b[38;5;241m=\u001b[39m get_best_discard(temp_hand, game)\n\u001b[1;32m     29\u001b[0m     draw_scores\u001b[38;5;241m.\u001b[39mappend(draw_score)\n\u001b[1;32m     30\u001b[0m expected_draw_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(draw_scores)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(draw_scores)\n",
      "File \u001b[0;32m/mnt/dl_mount/game_project/scoring.py:18\u001b[0m, in \u001b[0;36mget_best_discard\u001b[0;34m(hand, game, excluded_discard)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m excluded_discard \u001b[38;5;129;01mand\u001b[39;00m card \u001b[38;5;241m==\u001b[39m excluded_discard:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m score \u001b[38;5;241m=\u001b[39m score_hand(hand[:i] \u001b[38;5;241m+\u001b[39m hand[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:], game,\n\u001b[1;32m     19\u001b[0m                    keep_wild\u001b[38;5;241m=\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m game\u001b[38;5;241m.\u001b[39mis_going_out()))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m<\u001b[39m best_score:\n\u001b[1;32m     21\u001b[0m     best_score \u001b[38;5;241m=\u001b[39m score\n",
      "File \u001b[0;32m/mnt/dl_mount/game_project/scoring.py:67\u001b[0m, in \u001b[0;36mscore_hand\u001b[0;34m(hand, game, keep_wild)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m game\u001b[38;5;241m.\u001b[39mcard_value(card)\n\u001b[0;32m---> 67\u001b[0m lowest_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(score_card(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m hand)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Try every possible scoring subset for best score\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m window_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m, game\u001b[38;5;241m.\u001b[39mget_epoch()\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m/mnt/dl_mount/game_project/scoring.py:67\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m game\u001b[38;5;241m.\u001b[39mcard_value(card)\n\u001b[0;32m---> 67\u001b[0m lowest_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(score_card(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m hand)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Try every possible scoring subset for best score\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m window_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m, game\u001b[38;5;241m.\u001b[39mget_epoch()\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m/mnt/dl_mount/game_project/scoring.py:65\u001b[0m, in \u001b[0;36mscore_hand.<locals>.score_card\u001b[0;34m(card)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m card\u001b[38;5;241m.\u001b[39msuit() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJ\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m card\u001b[38;5;241m.\u001b[39mrank() \u001b[38;5;241m==\u001b[39m game\u001b[38;5;241m.\u001b[39mget_epoch():\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m game\u001b[38;5;241m.\u001b[39mcard_value(card)\n",
      "File \u001b[0;32m/mnt/dl_mount/game_project/five_crowns.py:118\u001b[0m, in \u001b[0;36mGame.card_value\u001b[0;34m(self, card)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m card\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m card\u001b[38;5;241m.\u001b[39mrank()\n",
      "File \u001b[0;32m/mnt/dl_mount/game_project/deck.py:21\u001b[0m, in \u001b[0;36mCard.rank\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_suit \u001b[38;5;241m=\u001b[39m suit\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hash \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__hash__\u001b[39m()\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrank\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rank\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msuit\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "payout_array=[]\n",
    "for episode in range(10000):\n",
    "    players = [dqn_player] + [GreedyPlayer(i) for i in range(1, agents)]\n",
    "    env = Game(players)\n",
    "    env.initialize_game()\n",
    "    state = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
    "    done = False\n",
    "    while not done:\n",
    "        for _ in range(agents):\n",
    "          env.play_round()\n",
    "          if env.is_game_over():\n",
    "            reward = payoff(env, 0)\n",
    "            payout_array.append(reward)\n",
    "            done = True\n",
    "          else:\n",
    "            reward = 0\n",
    "            done = False\n",
    "\n",
    "        next_state = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
    "\n",
    "        buffer.add((state, env._players[0].prev_action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        if buffer.size() >= batch_size:\n",
    "            batch = buffer.sample(batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "            actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "            next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "            # print(states.shape, actions.shape)\n",
    "            q_values = policy_net(states)\n",
    "            # print(q_values.shape)\n",
    "            # print(actions.shape)\n",
    "            q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "            next_q_values = target_net(next_states).max(1)[0]\n",
    "            target = rewards + (gamma * next_q_values * (1 - dones))\n",
    "\n",
    "            loss = nn.SmoothL1Loss()(q_values, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "            optimizer.step()\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episode {episode + 1}, Loss: {loss}, Win Rate: {(np.array(payout_array)==1).mean()}, Epsilon: {dqn_player.epsilon}\")\n",
    "        payout_array=[]\n",
    "    if episode % target_update_freq == 0:\n",
    "        '''\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*tau+target_net_state_dict[key]*(1-tau)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        '''\n",
    "        target_net.load_state_dict(target_net.state_dict())\n",
    "print(\"Ending loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRCjkJ5SVQoL"
   },
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), f'five_crowns_dqn_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_one_game():\n",
    "    \"\"\"\n",
    "    Simulate one game with given parameters\n",
    "    Return player 1 score\n",
    "    \"\"\"\n",
    "    players = [DQNPlayer(0,policy_net)] + [GreedyPlayer(i) for i in range(1, agents)]\n",
    "    players[0].epsilon = 0.05\n",
    "    dqn_player.hand=[]\n",
    "    game = Game(players=players, epoch=epoch)\n",
    "    game.initialize_game()\n",
    "\n",
    "    while not game.is_game_over():\n",
    "        game.play_round()\n",
    "\n",
    "    score = score_hand(players[0].hand, game)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 1000\n",
    "scores = [simulate_one_game() for _ in range(iters)]\n",
    "print(f\"Win Rate: {sum([1 for i in scores if i == 0])/iters}\")\n",
    "print(f\"Average Score: {sum(scores)/iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
