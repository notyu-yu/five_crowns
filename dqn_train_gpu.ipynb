{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCw04Buj6x2h"
   },
   "source": [
    "# Five Crowns Deep-Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwsXwXh360bJ"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WdPhYqtC6sZD"
   },
   "outputs": [],
   "source": [
    "#Reference: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "from copy import deepcopy\n",
    "from five_crowns import Game\n",
    "from greedy import GreedyPlayer\n",
    "from scoring import score_hand\n",
    "from state import State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ah3q5AFz61cb"
   },
   "source": [
    "### Define the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2honZ1hm65JP"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeLQuFxz66Dd"
   },
   "source": [
    "### Define the Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A2pekyat699v"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "prGV73NMClLe"
   },
   "outputs": [],
   "source": [
    "def payoff(game, current_player):\n",
    "    \"\"\"\n",
    "    Calculates the reward for the player at this state\n",
    "\n",
    "    Returns:\n",
    "        int: The reward for the current player in the state\n",
    "    \"\"\"\n",
    "    if not game._go_out:\n",
    "        return 0\n",
    "    # print(self.game.get_player_hand(self.curr_player_id))\n",
    "    hand_score = score_hand(\n",
    "        game.get_player_hand(current_player), game\n",
    "    )\n",
    "    return 1 if hand_score == 0 else -1\n",
    "\n",
    "def successor(self, game, action, num_players):\n",
    "    \"\"\"\n",
    "    Returns the successor state given the action from the current state\n",
    "\n",
    "    Args:\n",
    "        - action: The action to take from the current state\n",
    "\n",
    "    Returns:\n",
    "        - State: The new state after taking the action\n",
    "    \"\"\"\n",
    "    # Make a copy for the successor state\n",
    "    new_state = deepcopy(game)\n",
    "\n",
    "    # Get the next player\n",
    "    next_player_id = (game.get_active_player() + 1) % num_players\n",
    "    new_state._active_player = next_player_id\n",
    "\n",
    "    # Get the actions\n",
    "    first_action = action[0]\n",
    "    second_action = action[1]\n",
    "\n",
    "    # Execute the first action\n",
    "    added_card = None\n",
    "    if first_action == \"deck\":\n",
    "        added_card = new_state.get_deck().draw()\n",
    "        new_state.get_player_hand(game.get_active_player()).append(added_card)\n",
    "    elif first_action == \"discard\":\n",
    "        added_card = new_state.get_discard_pile().pop()\n",
    "        new_state.get_player_hand(game.get_active_player()).append(added_card)\n",
    "\n",
    "    # Execute the second action\n",
    "    if second_action == None:\n",
    "        new_state.get_player_hand(self.curr_player_id).remove(added_card)\n",
    "        new_state.get_discard_pile().append(added_card)\n",
    "    else:\n",
    "        new_state.get_player_hand(\n",
    "            self.curr_player_id).remove(second_action)\n",
    "        new_state.get_discard_pile().append(second_action)\n",
    "\n",
    "    # Check the hand score and update game ending conditions\n",
    "    # print(new_state.get_player_hand(self.curr_player_id))\n",
    "    hand_score = score_hand(\n",
    "        new_state.get_player_hand(self.curr_player_id), new_state\n",
    "    )\n",
    "    if hand_score == 0:\n",
    "        new_state._go_out = True\n",
    "        if new_state._remaining_players == new_state.num_players():\n",
    "            new_state._go_out_player = self.curr_player_id\n",
    "        new_state._remaining_players -= 1\n",
    "        if new_state._remaining_players == 0:\n",
    "            new_state._game_over = True\n",
    "\n",
    "    return State(new_state)\n",
    "\n",
    "def get_actions(self):\n",
    "      \"\"\"\n",
    "      Returns all possible actions from the current state\n",
    "      \"\"\"\n",
    "      if self.is_root:\n",
    "          actions = [(\"root\", c) for c in self.curr_player_hand if c != self.root_card]\n",
    "\n",
    "      else:\n",
    "          # initialize actions list\n",
    "          actions = []\n",
    "\n",
    "          if self.discard_pile_card:\n",
    "              #actions.append((\"discard\", self.discard_pile_card))\n",
    "              for card in self.curr_player_hand:\n",
    "                  actions.append((\"discard\", card))\n",
    "          if game.get_deck():\n",
    "              actions.append((\"deck\", None))\n",
    "              for card in self.curr_player_hand:\n",
    "                  actions.append((\"deck\", card))\n",
    "\n",
    "      return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mJ_6KTYOQ9bj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from deck import Card\n",
    "\n",
    "def card_to_idx(suit, rank):\n",
    "  suit_dict = {\n",
    "      'Clubs': 0,\n",
    "      'Diamonds': 1,\n",
    "      'Hearts': 2,\n",
    "      'Spades': 3,\n",
    "      'Stars': 4,\n",
    "      'J': 5\n",
    "  }\n",
    "\n",
    "  return 11 * suit_dict[suit] + (rank if suit != \"J\" else 0) - 3\n",
    "\n",
    "def idx_to_card(idx):\n",
    "    suit_dict = {\n",
    "        0: 'Clubs',\n",
    "        1: 'Diamonds',\n",
    "        2: 'Hearts',\n",
    "        3: 'Spades',\n",
    "        4: 'Stars',\n",
    "        5: 'J'\n",
    "    }\n",
    "\n",
    "    suit = suit_dict[idx // 11]\n",
    "    if suit == \"J\":\n",
    "        rank = 50\n",
    "    else:\n",
    "        rank = (idx % 11) + 3\n",
    "\n",
    "    return Card(rank, suit)\n",
    "\n",
    "def encode_state(num_players, full_deck, player_deck, discard_card, gone_out_status):\n",
    "    num_players = num_players\n",
    "\n",
    "    deck = set(full_deck)\n",
    "\n",
    "    encoded_deck = np.zeros(len(deck))\n",
    "    for idx, card in enumerate(player_deck):\n",
    "        card_idx = card_to_idx(card.suit(), card.rank())\n",
    "        encoded_deck[card_idx] += 1\n",
    "\n",
    "    # Encode discard card as (rank, suit)\n",
    "    discard_card_encoded = np.zeros(len(deck))\n",
    "    if discard_card is not None:\n",
    "        discard_idx = card_to_idx(discard_card.suit(), discard_card.rank())\n",
    "        discard_card_encoded[discard_idx] = 1\n",
    "\n",
    "    # Gone out status\n",
    "    gone_out_status_encoded = int(gone_out_status)\n",
    "\n",
    "    return np.concatenate([\n",
    "        encoded_deck,\n",
    "        discard_card_encoded,\n",
    "        [gone_out_status_encoded]\n",
    "    ])\n",
    "\n",
    "def inference(game, hand, discard_card, policy_net):\n",
    "    encoded_state = encode_state(game.num_players(), game.get_full_deck()._cards, hand, discard_card, game._go_out)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = policy_net(torch.Tensor(encoded_state).to(device)).to(\"cpu\").numpy()\n",
    "\n",
    "        sorted_list = [(output[i], idx_to_card(i)) for i in range(len(output))]\n",
    "        sorted_list.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        for _,card in sorted_list:\n",
    "            if card in hand and card != discard_card:\n",
    "                return card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Koa2H1ecSiEL"
   },
   "outputs": [],
   "source": [
    "from player import Player\n",
    "from scoring import get_best_discard\n",
    "from constants import GET_DISCARD, DRAW_CARD\n",
    "import copy\n",
    "\n",
    "class DQNPlayer(Player):\n",
    "    \"\"\"\n",
    "    DQN player always takes action that minimize score for turn\n",
    "    \"\"\"\n",
    "    def __init__(self, player_id, policy_net):\n",
    "        super().__init__(player_id)\n",
    "        self.prev_discard = None\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = 0.998\n",
    "        self.min_epsilon = 0.05\n",
    "        self.prev_action = None\n",
    "        self.policy_net = policy_net\n",
    "        self.policy_net.eval()\n",
    "\n",
    "    def draw_phase(self, game):\n",
    "        # Get best score if we take discard\n",
    "        new_card = game.get_discard_pile()[-1]\n",
    "        temp_hand = self.hand + [new_card]\n",
    "        _, discard_score = get_best_discard(temp_hand,game,excluded_discard=new_card)\n",
    "\n",
    "        # Get best expected score if we draw random\n",
    "        remaining_deck = copy.deepcopy(game.get_full_deck().get_cards())\n",
    "        draw_scores = []\n",
    "        for card in game.get_discard_pile() + self.hand:\n",
    "            remaining_deck.remove(card)\n",
    "        for card in remaining_deck:\n",
    "            temp_hand = self.hand + [card]\n",
    "            _, draw_score = get_best_discard(temp_hand, game)\n",
    "            draw_scores.append(draw_score)\n",
    "        expected_draw_score = sum(draw_scores)/len(draw_scores)\n",
    "\n",
    "        # Take action with better expected score\n",
    "        if discard_score < expected_draw_score:\n",
    "            self.prev_discard = game.get_discard_pile()[-1]\n",
    "            return GET_DISCARD\n",
    "        self.prev_discard = None\n",
    "        return DRAW_CARD\n",
    "\n",
    "    def discard_phase(self, game):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice([card for card in self.hand if card != self.prev_discard])\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = inference(game, self.hand, self.prev_discard, self.policy_net)\n",
    "\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        idx = card_to_idx(action.suit(), action.rank())\n",
    "        self.prev_action = idx\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dNWPTzBo_m8J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agents = 4\n",
    "state_dim = 113\n",
    "action_dim = 56\n",
    "epoch=3\n",
    "lr = 1e-4\n",
    "tau = 0.005\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnK0PDE_pwJ8",
    "outputId": "1782bab7-7c25-4551-85f6-82e8e4efa098"
   },
   "outputs": [],
   "source": [
    "policy_net = DQN(state_dim, action_dim).to(device)\n",
    "#policy_net.load_state_dict(torch.load(f'five_crowns_dqn_{epoch}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "2FNRG0_fpv8W"
   },
   "outputs": [],
   "source": [
    "target_net = DQN(state_dim, action_dim).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "dqn_player = DQNPlayer(0, policy_net)\n",
    "players = [dqn_player] + [GreedyPlayer(i) for i in range(1, agents)]\n",
    "\n",
    "env = Game(players,epoch=epoch)\n",
    "env.initialize_game()\n",
    "\n",
    "state_encoded = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr, amsgrad=True)\n",
    "buffer = ReplayBuffer(10000)\n",
    "batch_size = 128\n",
    "gamma = 0.99\n",
    "target_update_freq = 100\n",
    "loss = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgFWGvHJVNkf"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQCgN6l1VM1t",
    "outputId": "ba21aefe-e3b3-43d3-f12b-8e62db965b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Loss: 0.12566494941711426, Win Rate: 0.0, Epsilon: 0.05\n",
      "Episode 1001, Loss: 0.14582619071006775, Win Rate: 0.24465648854961833, Epsilon: 0.05\n",
      "Episode 2001, Loss: 0.15195342898368835, Win Rate: 0.26347760060744113, Epsilon: 0.05\n",
      "Episode 3001, Loss: 0.13204559683799744, Win Rate: 0.2552142586272279, Epsilon: 0.05\n",
      "Episode 4001, Loss: 0.12778624892234802, Win Rate: 0.2519201228878648, Epsilon: 0.05\n",
      "Episode 5001, Loss: 0.1049223467707634, Win Rate: 0.2569496619083396, Epsilon: 0.05\n",
      "Episode 6001, Loss: 0.13794571161270142, Win Rate: 0.25696830851470026, Epsilon: 0.05\n",
      "Episode 7001, Loss: 0.1321375072002411, Win Rate: 0.2684899845916795, Epsilon: 0.05\n",
      "Episode 8001, Loss: 0.13473954796791077, Win Rate: 0.27864484202512374, Epsilon: 0.05\n",
      "Episode 9001, Loss: 0.14906473457813263, Win Rate: 0.2816358024691358, Epsilon: 0.05\n",
      "Ending loss:  0.12944823503494263\n"
     ]
    }
   ],
   "source": [
    "payout_array=[]\n",
    "for episode in range(10000):\n",
    "    players = [dqn_player] + [GreedyPlayer(i) for i in range(1, agents)]\n",
    "    env = Game(players)\n",
    "    env.initialize_game()\n",
    "    state = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
    "    done = False\n",
    "    while not done:\n",
    "        for _ in range(agents):\n",
    "          env.play_round()\n",
    "          if env.is_game_over():\n",
    "            reward = payoff(env, 0)\n",
    "            payout_array.append(reward)\n",
    "            done = True\n",
    "          else:\n",
    "            reward = 0\n",
    "            done = False\n",
    "\n",
    "        next_state = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
    "\n",
    "        buffer.add((state, env._players[0].prev_action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        if buffer.size() >= batch_size:\n",
    "            batch = buffer.sample(batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "            actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "            next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "            # print(states.shape, actions.shape)\n",
    "            q_values = policy_net(states)\n",
    "            # print(q_values.shape)\n",
    "            # print(actions.shape)\n",
    "            q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "            next_q_values = target_net(next_states).max(1)[0]\n",
    "            target = rewards + (gamma * next_q_values * (1 - dones))\n",
    "\n",
    "            loss = nn.SmoothL1Loss()(q_values, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "            optimizer.step()\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episode {episode + 1}, Loss: {loss}, Win Rate: {(np.array(payout_array)==1).mean()}, Epsilon: {dqn_player.epsilon}\")\n",
    "        payout_array=[]\n",
    "    if episode % target_update_freq == 0:\n",
    "        '''\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*tau+target_net_state_dict[key]*(1-tau)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        '''\n",
    "        target_net.load_state_dict(target_net.state_dict())\n",
    "print(\"Ending loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "xRCjkJ5SVQoL"
   },
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), f'five_crowns_dqn_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_one_game():\n",
    "    \"\"\"\n",
    "    Simulate one game with given parameters\n",
    "    Return player 1 score\n",
    "    \"\"\"\n",
    "    players = [dqn_player] + [GreedyPlayer(i) for i in range(1, agents)]\n",
    "    dqn_player.hand=[]\n",
    "    game = Game(players=players, epoch=epoch)\n",
    "    game.initialize_game()\n",
    "\n",
    "    while not game.is_game_over():\n",
    "        game.play_round()\n",
    "\n",
    "    score = score_hand(players[0].hand, game)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 10000\n",
    "scores = [simulate_one_game() for _ in range(iters)]\n",
    "print(f\"Win Rate: {sum([1 for i in scores if i == 0])/iters}\")\n",
    "print(f\"Average Score: {sum(scores)/iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
