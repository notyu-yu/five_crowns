{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCw04Buj6x2h"
   },
   "source": [
    "# Five Crowns Deep-Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwsXwXh360bJ"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WdPhYqtC6sZD"
   },
   "outputs": [],
   "source": [
    "#Reference: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "from copy import deepcopy\n",
    "from five_crowns import Game\n",
    "from greedy import GreedyPlayer\n",
    "from scoring import score_hand\n",
    "from state import State\n",
    "from dqn import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ah3q5AFz61cb"
   },
   "source": [
    "### Define the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeLQuFxz66Dd"
   },
   "source": [
    "### Define the Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A2pekyat699v"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "prGV73NMClLe"
   },
   "outputs": [],
   "source": [
    "def payoff(game, current_player):\n",
    "    \"\"\"\n",
    "    Calculates the reward for the player at this state\n",
    "\n",
    "    Returns:\n",
    "        int: The reward for the current player in the state\n",
    "    \"\"\"\n",
    "    if not game._go_out:\n",
    "        return 0\n",
    "    # print(self.game.get_player_hand(self.curr_player_id))\n",
    "    hand_score = score_hand(\n",
    "        game.get_player_hand(current_player), game\n",
    "    )\n",
    "    return 10 if hand_score == 0 else -1*max(hand_score,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mJ_6KTYOQ9bj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from deck import Card\n",
    "\n",
    "def card_to_idx(suit, rank):\n",
    "  suit_dict = {\n",
    "      'Clubs': 0,\n",
    "      'Diamonds': 1,\n",
    "      'Hearts': 2,\n",
    "      'Spades': 3,\n",
    "      'Stars': 4,\n",
    "      'J': 5\n",
    "  }\n",
    "\n",
    "  return 11 * suit_dict[suit] + (rank if suit != \"J\" else 0) - 3\n",
    "\n",
    "def idx_to_card(idx):\n",
    "    suit_dict = {\n",
    "        0: 'Clubs',\n",
    "        1: 'Diamonds',\n",
    "        2: 'Hearts',\n",
    "        3: 'Spades',\n",
    "        4: 'Stars',\n",
    "        5: 'J'\n",
    "    }\n",
    "\n",
    "    suit = suit_dict[idx // 11]\n",
    "    if suit == \"J\":\n",
    "        rank = 50\n",
    "    else:\n",
    "        rank = (idx % 11) + 3\n",
    "\n",
    "    return Card(rank, suit)\n",
    "\n",
    "def encode_state(num_players, full_deck, player_deck, discard_card, gone_out_status):\n",
    "    num_players = num_players\n",
    "\n",
    "    deck = set(full_deck)\n",
    "\n",
    "    encoded_deck = np.zeros(len(deck))\n",
    "    for idx, card in enumerate(player_deck):\n",
    "        card_idx = card_to_idx(card.suit(), card.rank())\n",
    "        encoded_deck[card_idx] += 1\n",
    "\n",
    "    # Encode discard card as (rank, suit)\n",
    "    discard_card_encoded = np.zeros(len(deck))\n",
    "    if discard_card is not None:\n",
    "        discard_idx = card_to_idx(discard_card.suit(), discard_card.rank())\n",
    "        discard_card_encoded[discard_idx] = 1\n",
    "\n",
    "    # Gone out status\n",
    "    gone_out_status_encoded = int(gone_out_status)\n",
    "\n",
    "    return np.concatenate([\n",
    "        encoded_deck,\n",
    "        discard_card_encoded,\n",
    "        [gone_out_status_encoded]\n",
    "    ])\n",
    "\n",
    "def inference(game, hand, discard_card, policy_net):\n",
    "    encoded_state = encode_state(game.num_players(), game.get_full_deck()._cards, hand, discard_card, game._go_out)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = policy_net(torch.Tensor(encoded_state).to(device)).to(\"cpu\").numpy()\n",
    "\n",
    "        sorted_list = [(output[i], idx_to_card(i)) for i in range(len(output))]\n",
    "        sorted_list.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        for _,card in sorted_list:\n",
    "            if card in hand and card != discard_card:\n",
    "                return card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Koa2H1ecSiEL"
   },
   "outputs": [],
   "source": [
    "from player import Player\n",
    "from scoring import get_best_discard\n",
    "from constants import GET_DISCARD, DRAW_CARD\n",
    "import copy\n",
    "\n",
    "class DQNPlayer(Player):\n",
    "    \"\"\"\n",
    "    DQN player always takes action that minimize score for turn\n",
    "    \"\"\"\n",
    "    def __init__(self, player_id, policy_net):\n",
    "        super().__init__(player_id)\n",
    "        self.prev_discard = None\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.997\n",
    "        self.min_epsilon = 0.05\n",
    "        self.prev_action = None\n",
    "        self.policy_net = policy_net\n",
    "        self.policy_net.eval()\n",
    "\n",
    "    def draw_phase(self, game):\n",
    "        # Get best score if we take discard\n",
    "        new_card = game.get_discard_pile()[-1]\n",
    "        temp_hand = self.hand + [new_card]\n",
    "        _, discard_score = get_best_discard(temp_hand,game,excluded_discard=new_card)\n",
    "\n",
    "        # Get best expected score if we draw random\n",
    "        remaining_deck = copy.deepcopy(game.get_full_deck().get_cards())\n",
    "        draw_scores = []\n",
    "        for card in game.get_discard_pile() + self.hand:\n",
    "            remaining_deck.remove(card)\n",
    "        for card in remaining_deck:\n",
    "            temp_hand = self.hand + [card]\n",
    "            _, draw_score = get_best_discard(temp_hand, game)\n",
    "            draw_scores.append(draw_score)\n",
    "        expected_draw_score = sum(draw_scores)/len(draw_scores)\n",
    "\n",
    "        # Take action with better expected score\n",
    "        if discard_score < expected_draw_score:\n",
    "            self.prev_discard = game.get_discard_pile()[-1]\n",
    "            return GET_DISCARD\n",
    "        self.prev_discard = None\n",
    "        return DRAW_CARD\n",
    "\n",
    "    def discard_phase(self, game):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice([card for card in self.hand if card != self.prev_discard])\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = inference(game, self.hand, self.prev_discard, self.policy_net)\n",
    "\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        idx = card_to_idx(action.suit(), action.rank())\n",
    "        self.prev_action = idx\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dNWPTzBo_m8J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agents = 4\n",
    "train_agents = 4\n",
    "state_dim = 113\n",
    "action_dim = 56\n",
    "epoch=3\n",
    "lr = 1e-4\n",
    "tau = 0.005\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnK0PDE_pwJ8",
    "outputId": "1782bab7-7c25-4551-85f6-82e8e4efa098"
   },
   "outputs": [],
   "source": [
    "policy_net = DQN(state_dim, action_dim).to(device)\n",
    "#policy_net.load_state_dict(torch.load(f'five_crowns_dqn_{epoch}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2FNRG0_fpv8W"
   },
   "outputs": [],
   "source": [
    "target_net = DQN(state_dim, action_dim).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "dqn_player = DQNPlayer(0, policy_net)\n",
    "#dqn_player.epsilon = 0.05\n",
    "players = [dqn_player] + [GreedyPlayer(i) for i in range(1, agents)]\n",
    "\n",
    "env = Game(players,epoch=epoch)\n",
    "env.initialize_game()\n",
    "\n",
    "state_encoded = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr, amsgrad=True)\n",
    "buffer = ReplayBuffer(10000)\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "target_update_freq = 200\n",
    "loss = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgFWGvHJVNkf"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQCgN6l1VM1t",
    "outputId": "ba21aefe-e3b3-43d3-f12b-8e62db965b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Loss: 10.3065185546875, Win Rate: 0.0, Epsilon: 0.05\n",
      "Episode 1001, Loss: 14.379650115966797, Win Rate: 0.23012869038607117, Epsilon: 0.05\n",
      "Episode 2001, Loss: 12.582551002502441, Win Rate: 0.2159392789373814, Epsilon: 0.05\n",
      "Episode 3001, Loss: 12.336451530456543, Win Rate: 0.23893805309734514, Epsilon: 0.05\n",
      "Episode 4001, Loss: 13.626739501953125, Win Rate: 0.22920587128340233, Epsilon: 0.05\n",
      "Episode 5001, Loss: 12.473556518554688, Win Rate: 0.2267616191904048, Epsilon: 0.05\n",
      "Episode 6001, Loss: 12.616636276245117, Win Rate: 0.21721611721611722, Epsilon: 0.05\n",
      "Episode 7001, Loss: 10.456879615783691, Win Rate: 0.2314018691588785, Epsilon: 0.05\n",
      "Episode 8001, Loss: 14.5792236328125, Win Rate: 0.23117076808351977, Epsilon: 0.05\n",
      "Episode 9001, Loss: 11.24612045288086, Win Rate: 0.22846580406654343, Epsilon: 0.05\n",
      "Episode 10001, Loss: 15.437291145324707, Win Rate: 0.2170978627671541, Epsilon: 0.05\n",
      "Episode 11001, Loss: 15.311363220214844, Win Rate: 0.22628571428571428, Epsilon: 0.05\n",
      "Episode 12001, Loss: 13.319305419921875, Win Rate: 0.22598454177401545, Epsilon: 0.05\n",
      "Episode 13001, Loss: 15.196077346801758, Win Rate: 0.21551081282624907, Epsilon: 0.05\n",
      "Episode 14001, Loss: 11.435547828674316, Win Rate: 0.2386237513873474, Epsilon: 0.05\n",
      "Episode 15001, Loss: 13.239362716674805, Win Rate: 0.23400995787054768, Epsilon: 0.05\n",
      "Episode 16001, Loss: 13.977307319641113, Win Rate: 0.2224727546035325, Epsilon: 0.05\n",
      "Episode 17001, Loss: 12.506681442260742, Win Rate: 0.2554144884241972, Epsilon: 0.05\n",
      "Episode 18001, Loss: 14.032159805297852, Win Rate: 0.2370142362447095, Epsilon: 0.05\n",
      "Episode 19001, Loss: 11.19493293762207, Win Rate: 0.21217712177121772, Epsilon: 0.05\n",
      "Episode 20001, Loss: 10.329571723937988, Win Rate: 0.23622047244094488, Epsilon: 0.05\n",
      "Episode 21001, Loss: 14.224411964416504, Win Rate: 0.21868050407709413, Epsilon: 0.05\n",
      "Episode 22001, Loss: 11.484051704406738, Win Rate: 0.24496006085964245, Epsilon: 0.05\n",
      "Episode 23001, Loss: 11.033272743225098, Win Rate: 0.2128787878787879, Epsilon: 0.05\n",
      "Episode 24001, Loss: 13.00139045715332, Win Rate: 0.22539334065129893, Epsilon: 0.05\n",
      "Episode 25001, Loss: 12.692808151245117, Win Rate: 0.2291899962672639, Epsilon: 0.05\n",
      "Episode 26001, Loss: 10.2269287109375, Win Rate: 0.2013788098693759, Epsilon: 0.05\n",
      "Episode 27001, Loss: 11.472478866577148, Win Rate: 0.22536258832279657, Epsilon: 0.05\n",
      "Episode 28001, Loss: 13.390920639038086, Win Rate: 0.2205770690964313, Epsilon: 0.05\n",
      "Episode 29001, Loss: 12.708599090576172, Win Rate: 0.22720478325859492, Epsilon: 0.05\n",
      "Episode 30001, Loss: 14.007112503051758, Win Rate: 0.23717948717948717, Epsilon: 0.05\n",
      "Episode 31001, Loss: 20.307588577270508, Win Rate: 0.22668153102935712, Epsilon: 0.05\n",
      "Episode 32001, Loss: 13.520553588867188, Win Rate: 0.2320211960635882, Epsilon: 0.05\n",
      "Episode 33001, Loss: 15.88255500793457, Win Rate: 0.2312792511700468, Epsilon: 0.05\n",
      "Episode 34001, Loss: 17.36371612548828, Win Rate: 0.22777147181233448, Epsilon: 0.05\n",
      "Episode 35001, Loss: 13.274070739746094, Win Rate: 0.21974758723088345, Epsilon: 0.05\n",
      "Episode 36001, Loss: 9.214372634887695, Win Rate: 0.23635679337598794, Epsilon: 0.05\n",
      "Episode 37001, Loss: 12.228068351745605, Win Rate: 0.2217651458489155, Epsilon: 0.05\n",
      "Episode 38001, Loss: 10.865425109863281, Win Rate: 0.20848845867460908, Epsilon: 0.05\n",
      "Episode 39001, Loss: 10.37582015991211, Win Rate: 0.25096227867590454, Epsilon: 0.05\n",
      "Episode 40001, Loss: 5.902194976806641, Win Rate: 0.2396598376497874, Epsilon: 0.05\n",
      "Episode 41001, Loss: 12.61741828918457, Win Rate: 0.22485422740524783, Epsilon: 0.05\n",
      "Episode 42001, Loss: 16.668033599853516, Win Rate: 0.22095167834747326, Epsilon: 0.05\n",
      "Episode 43001, Loss: 15.89464282989502, Win Rate: 0.2505677517032551, Epsilon: 0.05\n",
      "Episode 44001, Loss: 12.352167129516602, Win Rate: 0.21048603138155378, Epsilon: 0.05\n",
      "Episode 45001, Loss: 11.401543617248535, Win Rate: 0.22816265060240964, Epsilon: 0.05\n",
      "Episode 46001, Loss: 18.368865966796875, Win Rate: 0.25113464447806355, Epsilon: 0.05\n",
      "Episode 47001, Loss: 16.17810821533203, Win Rate: 0.23699851411589895, Epsilon: 0.05\n",
      "Episode 48001, Loss: 14.16537094116211, Win Rate: 0.24455483377913642, Epsilon: 0.05\n",
      "Episode 49001, Loss: 10.675241470336914, Win Rate: 0.22333077513430544, Epsilon: 0.05\n",
      "Episode 50001, Loss: 15.176103591918945, Win Rate: 0.24860646599777034, Epsilon: 0.05\n",
      "Episode 51001, Loss: 12.255111694335938, Win Rate: 0.22302158273381295, Epsilon: 0.05\n",
      "Episode 52001, Loss: 13.685694694519043, Win Rate: 0.24183514774494558, Epsilon: 0.05\n",
      "Episode 53001, Loss: 16.625717163085938, Win Rate: 0.2459078797106966, Epsilon: 0.05\n",
      "Episode 54001, Loss: 11.374585151672363, Win Rate: 0.24300498275201227, Epsilon: 0.05\n",
      "Episode 55001, Loss: 11.840119361877441, Win Rate: 0.23928293063133282, Epsilon: 0.05\n",
      "Episode 56001, Loss: 13.768945693969727, Win Rate: 0.21109861267341581, Epsilon: 0.05\n",
      "Episode 57001, Loss: 13.141632080078125, Win Rate: 0.22338438550616363, Epsilon: 0.05\n",
      "Episode 58001, Loss: 14.139358520507812, Win Rate: 0.23324789454412304, Epsilon: 0.05\n",
      "Episode 59001, Loss: 15.922903060913086, Win Rate: 0.2337018680899733, Epsilon: 0.05\n",
      "Episode 60001, Loss: 11.599889755249023, Win Rate: 0.23784592370979807, Epsilon: 0.05\n",
      "Episode 61001, Loss: 13.144872665405273, Win Rate: 0.24971537001897534, Epsilon: 0.05\n",
      "Episode 62001, Loss: 9.91213607788086, Win Rate: 0.22744220730797912, Epsilon: 0.05\n",
      "Episode 63001, Loss: 16.26775550842285, Win Rate: 0.21892602328201277, Epsilon: 0.05\n",
      "Episode 64001, Loss: 9.933000564575195, Win Rate: 0.23829463266082984, Epsilon: 0.05\n",
      "Episode 65001, Loss: 12.801448822021484, Win Rate: 0.2537142857142857, Epsilon: 0.05\n",
      "Episode 66001, Loss: 12.675874710083008, Win Rate: 0.24971363115693013, Epsilon: 0.05\n",
      "Episode 67001, Loss: 12.947123527526855, Win Rate: 0.23261032161555723, Epsilon: 0.05\n",
      "Episode 68001, Loss: 14.494696617126465, Win Rate: 0.2368815592203898, Epsilon: 0.05\n",
      "Episode 69001, Loss: 17.531715393066406, Win Rate: 0.24530428249436514, Epsilon: 0.05\n",
      "Episode 70001, Loss: 15.221364974975586, Win Rate: 0.23691353237689028, Epsilon: 0.05\n",
      "Episode 71001, Loss: 15.488676071166992, Win Rate: 0.21925535915757804, Epsilon: 0.05\n",
      "Episode 72001, Loss: 16.92380142211914, Win Rate: 0.22793296089385476, Epsilon: 0.05\n",
      "Episode 73001, Loss: 17.35519790649414, Win Rate: 0.22255639097744362, Epsilon: 0.05\n",
      "Episode 74001, Loss: 14.357686996459961, Win Rate: 0.23367308418271046, Epsilon: 0.05\n",
      "Episode 75001, Loss: 14.574151992797852, Win Rate: 0.22102124487513977, Epsilon: 0.05\n",
      "Episode 76001, Loss: 13.046446800231934, Win Rate: 0.23132621951219512, Epsilon: 0.05\n",
      "Episode 77001, Loss: 15.682008743286133, Win Rate: 0.22374768089053804, Epsilon: 0.05\n",
      "Episode 78001, Loss: 16.03790283203125, Win Rate: 0.23751387347391786, Epsilon: 0.05\n",
      "Episode 79001, Loss: 12.133859634399414, Win Rate: 0.23621148725751237, Epsilon: 0.05\n",
      "Episode 80001, Loss: 10.660390853881836, Win Rate: 0.2403400309119011, Epsilon: 0.05\n",
      "Episode 81001, Loss: 13.553707122802734, Win Rate: 0.24396782841823056, Epsilon: 0.05\n",
      "Episode 82001, Loss: 10.506385803222656, Win Rate: 0.2376865671641791, Epsilon: 0.05\n",
      "Episode 83001, Loss: 19.34117889404297, Win Rate: 0.25822494261667944, Epsilon: 0.05\n",
      "Episode 84001, Loss: 13.82730484008789, Win Rate: 0.24943052391799544, Epsilon: 0.05\n",
      "Episode 85001, Loss: 13.861953735351562, Win Rate: 0.22708411920030178, Epsilon: 0.05\n",
      "Episode 86001, Loss: 12.964988708496094, Win Rate: 0.226737561716673, Epsilon: 0.05\n",
      "Episode 87001, Loss: 14.092460632324219, Win Rate: 0.24776453055141578, Epsilon: 0.05\n",
      "Episode 88001, Loss: 11.688400268554688, Win Rate: 0.2493525712171661, Epsilon: 0.05\n",
      "Episode 89001, Loss: 12.09695053100586, Win Rate: 0.2376275028333963, Epsilon: 0.05\n",
      "Episode 90001, Loss: 10.943744659423828, Win Rate: 0.232319391634981, Epsilon: 0.05\n",
      "Episode 91001, Loss: 11.443269729614258, Win Rate: 0.2259651778955337, Epsilon: 0.05\n",
      "Episode 92001, Loss: 12.600778579711914, Win Rate: 0.22850592281238058, Epsilon: 0.05\n",
      "Episode 93001, Loss: 17.645200729370117, Win Rate: 0.23470149253731343, Epsilon: 0.05\n",
      "Episode 94001, Loss: 14.79745101928711, Win Rate: 0.236754353464246, Epsilon: 0.05\n",
      "Episode 95001, Loss: 13.372097969055176, Win Rate: 0.2352716143840857, Epsilon: 0.05\n",
      "Episode 96001, Loss: 14.532251358032227, Win Rate: 0.2421325435024065, Epsilon: 0.05\n",
      "Episode 97001, Loss: 9.468257904052734, Win Rate: 0.2580037664783427, Epsilon: 0.05\n",
      "Episode 98001, Loss: 14.57750415802002, Win Rate: 0.2117952522255193, Epsilon: 0.05\n",
      "Episode 99001, Loss: 12.714374542236328, Win Rate: 0.2544356360890902, Epsilon: 0.05\n",
      "Ending loss:  11.00915813446045\n"
     ]
    }
   ],
   "source": [
    "payout_array=[]\n",
    "for episode in range(100000):\n",
    "    players = [dqn_player] + [GreedyPlayer(i) for i in range(1, train_agents)]\n",
    "    env = Game(players,epoch=epoch)\n",
    "    env.initialize_game()\n",
    "    state = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
    "    done = False\n",
    "    while not done:\n",
    "        for _ in range(agents):\n",
    "          env.play_round()\n",
    "          if env.is_game_over():\n",
    "            reward = payoff(env, 0)\n",
    "            payout_array.append(reward)\n",
    "            done = True\n",
    "          else:\n",
    "            reward = 0\n",
    "            done = False\n",
    "\n",
    "        next_state = encode_state(agents, env.get_full_deck()._cards, env._players[0].hand, env._discard_pile[-1], 0)\n",
    "\n",
    "        buffer.add((state, env._players[0].prev_action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        if buffer.size() >= batch_size:\n",
    "            batch = buffer.sample(batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "            actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "            next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "            # print(states.shape, actions.shape)\n",
    "            q_values = policy_net(states)\n",
    "            # print(q_values.shape)\n",
    "            # print(actions.shape)\n",
    "            q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "            next_q_values = target_net(next_states).max(1)[0]\n",
    "            target = rewards + (gamma * next_q_values * (1 - dones))\n",
    "\n",
    "            loss = nn.SmoothL1Loss()(q_values, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "            optimizer.step()\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episode {episode + 1}, Loss: {loss}, Win Rate: {(np.array(payout_array)>=1).mean()}, Epsilon: {dqn_player.epsilon}\")\n",
    "        payout_array=[]\n",
    "    if episode % target_update_freq == 0:\n",
    "        '''\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*tau+target_net_state_dict[key]*(1-tau)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        '''\n",
    "        target_net.load_state_dict(target_net.state_dict())\n",
    "print(\"Ending loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "xRCjkJ5SVQoL"
   },
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), f'five_crowns_dqn_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_one_game():\n",
    "    \"\"\"\n",
    "    Simulate one game with given parameters\n",
    "    Return player 1 score\n",
    "    \"\"\"\n",
    "    players = [DQNPlayer(0,policy_net)] + [GreedyPlayer(i) for i in range(1,agents)]\n",
    "    players[0].epsilon = 0.05\n",
    "    game = Game(players=players, epoch=epoch)\n",
    "    game.initialize_game()\n",
    "\n",
    "    while not game.is_game_over():\n",
    "        game.play_round()\n",
    "\n",
    "    score = score_hand(players[0].hand, game)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game for Epoch 3\n",
      "Win Rate: 0.3721\n",
      "Average Score: 18.5485\n"
     ]
    }
   ],
   "source": [
    "iters = 10000\n",
    "scores = [simulate_one_game() for _ in range(iters)]\n",
    "print(f\"Game for Epoch {epoch}\")\n",
    "print(f\"Win Rate: {sum([1 for i in scores if i == 0])/iters}\")\n",
    "print(f\"Average Score: {sum(scores)/iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
